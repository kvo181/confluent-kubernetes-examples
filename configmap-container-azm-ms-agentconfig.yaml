apiVersion: v1
data:
  agent-settings: |-
    # prometheus scrape fluent bit settings for high scale
    # buffer size should be greater than or equal to chunk size else we set it to chunk size.
    [agent_settings.prometheus_fbit_settings]
      tcp_listener_chunk_size = 10
      tcp_listener_buffer_size = 10
      tcp_listener_mem_buf_limit = 200

    # The following settings are "undocumented", we don't recommend uncommenting them unless directed by Microsoft.
    # They increase the maximum stdout/stderr log collection rate but will also cause higher cpu/memory usage.
    # [agent_settings.fbit_config]
    #   log_flush_interval_secs = "1"                 # default value is 15
    #   tail_mem_buf_limit_megabytes = "10"           # default value is 10
    #   tail_buf_chunksize_megabytes = "1"            # default value is 32kb (comment out this line for default)
    #   tail_buf_maxsize_megabytes = "1"              # defautl value is 32kb (comment out this line for default)
  alertable-metrics-configuration-settings: |-
    # Alertable metrics configuration settings for container resource utilization
    [alertable_metrics_configuration_settings.container_resource_utilization_thresholds]
        # The threshold(Type Float) will be rounded off to 2 decimal points
        # Threshold for container cpu, metric will be sent only when cpu utilization exceeds or becomes equal to the following percentage
        container_cpu_threshold_percentage = 95.0
        # Threshold for container memoryRss, metric will be sent only when memory rss exceeds or becomes equal to the following percentage
        container_memory_rss_threshold_percentage = 95.0
        # Threshold for container memoryWorkingSet, metric will be sent only when memory working set exceeds or becomes equal to the following percentage
        container_memory_working_set_threshold_percentage = 95.0

    # Alertable metrics configuration settings for persistent volume utilization
    [alertable_metrics_configuration_settings.pv_utilization_thresholds]
        # Threshold for persistent volume usage bytes, metric will be sent only when persistent volume utilization exceeds or becomes equal to the following percentage
        pv_usage_threshold_percentage = 60.0

    # Alertable metrics configuration settings for completed jobs count
    [alertable_metrics_configuration_settings.job_completion_threshold]
        # Threshold for completed job count , metric will be sent only for those jobs which were completed earlier than the following threshold
        job_completion_threshold_time_minutes = 360
  config-version: ver1
  integrations: |-
    [integrations.azure_network_policy_manager]
        collect_basic_metrics = false
        collect_advanced_metrics = false
  log-data-collection-settings: |-
    # Log data collection settings
    # Any errors related to config map settings can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.

    [log_collection_settings]
       [log_collection_settings.stdout]
          # In the absense of this configmap, default value for enabled is true
          enabled = true
          # exclude_namespaces setting holds good only if enabled is set to true
          # kube-system log collection is disabled by default in the absence of 'log_collection_settings.stdout' setting. If you want to enable kube-system, remove it from the following setting.
          # If you want to continue to disable kube-system log collection keep this namespace in the following setting and add any other namespace you want to disable log collection to the array.
          # In the absense of this configmap, default value for exclude_namespaces = ["kube-system"]
          exclude_namespaces = ["kube-system"]

       [log_collection_settings.stderr]
          # Default value for enabled is true
          enabled = true
          # exclude_namespaces setting holds good only if enabled is set to true
          # kube-system log collection is disabled by default in the absence of 'log_collection_settings.stderr' setting. If you want to enable kube-system, remove it from the following setting.
          # If you want to continue to disable kube-system log collection keep this namespace in the following setting and add any other namespace you want to disable log collection to the array.
          # In the absense of this cofigmap, default value for exclude_namespaces = ["kube-system"]
          exclude_namespaces = ["kube-system"]

       [log_collection_settings.env_var]
          # In the absense of this configmap, default value for enabled is true
          enabled = true
       [log_collection_settings.enrich_container_logs]
          # In the absense of this configmap, default value for enrich_container_logs is false
          enabled = false
          # When this is enabled (enabled = true), every container log entry (both stdout & stderr) will be enriched with container Name & container Image
       [log_collection_settings.collect_all_kube_events]
          # In the absense of this configmap, default value for collect_all_kube_events is false
          # When the setting is set to false, only the kube events with !normal event type will be collected
          enabled = false
          # When this is enabled (enabled = true), all kube events including normal events will be collected
  metric_collection_settings: |-
    # Metrics collection settings for metrics sent to Log Analytics and MDM
    [metric_collection_settings.collect_kube_system_pv_metrics]
      # In the absense of this configmap, default value for collect_kube_system_pv_metrics is false
      # When the setting is set to false, only the persistent volume metrics outside the kube-system namespace will be collected
      enabled = false
      # When this is enabled (enabled = true), persistent volume metrics including those in the kube-system namespace will be collected
  prometheus-data-collection-settings: |-
    # Custom Prometheus metrics data collection settings
    [prometheus_data_collection_settings.cluster]
        # Cluster level scrape endpoint(s). These metrics will be scraped from agent's Replicaset (singleton)
        # Any errors related to prometheus scraping can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.

        #Interval specifying how often to scrape for metrics. This is duration of time and can be specified for supporting settings by combining an integer value and time unit as a string value. Valid time units are ns, us (or µs), ms, s, m, h.
        interval = "5m"

        ## Uncomment the following settings with valid string arrays for prometheus scraping
        #fieldpass = ["metric_to_pass1", "metric_to_pass12"]

        #fielddrop = ["metric_to_drop"]

        # An array of urls to scrape metrics from.
        # urls = ["http://localhost:8093/metrics"]

        # An array of Kubernetes services to scrape metrics from.
        kubernetes_services = ["http://jms-adapter-cip-service.connectors:8093/metrics"]

        # When monitor_kubernetes_pods = true, replicaset will scrape Kubernetes pods for the following prometheus annotations:
        # - prometheus.io/scrape: Enable scraping for this pod
        # - prometheus.io/scheme: If the metrics endpoint is secured then you will need to
        #     set this to `https` & most likely set the tls config.
        # - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.
        # - prometheus.io/port: If port is not 9102 use this annotation
        monitor_kubernetes_pods = false

        ## Restricts Kubernetes monitoring to namespaces for pods that have annotations set and are scraped using the monitor_kubernetes_pods setting.
        ## This will take effect when monitor_kubernetes_pods is set to true
        ##   ex: monitor_kubernetes_pods_namespaces = ["default1", "default2", "default3"]
        # monitor_kubernetes_pods_namespaces = ["default1"]

        ## Label selector to target pods which have the specified label
        ## This will take effect when monitor_kubernetes_pods is set to true
        ## Reference the docs at https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors
        # kubernetes_label_selector = "env=dev,app=nginx"

        ## Field selector to target pods which have the specified field
        ## This will take effect when monitor_kubernetes_pods is set to true
        ## Reference the docs at https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/
        ## eg. To scrape pods on a specific node
        # kubernetes_field_selector = "spec.nodeName=$HOSTNAME"

    [prometheus_data_collection_settings.node]
        # Node level scrape endpoint(s). These metrics will be scraped from agent's DaemonSet running in every node in the cluster
        # Any errors related to prometheus scraping can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.

        #Interval specifying how often to scrape for metrics. This is duration of time and can be specified for supporting settings by combining an integer value and time unit as a string value. Valid time units are ns, us (or µs), ms, s, m, h.
        interval = "1m"

        ## Uncomment the following settings with valid string arrays for prometheus scraping

        # An array of urls to scrape metrics from. $NODE_IP (all upper case) will substitute of running Node's IP address
        # urls = ["http://$NODE_IP:9103/metrics"]

        #fieldpass = ["metric_to_pass1", "metric_to_pass12"]

        #fielddrop = ["metric_to_drop"]
  schema-version: v1
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"agent-settings":"# prometheus scrape fluent bit settings for high scale\n# buffer size should be greater than or equal to chunk size else we set it to chunk size.\n[agent_settings.prometheus_fbit_settings]\n  tcp_listener_chunk_size = 10\n  tcp_listener_buffer_size = 10\n  tcp_listener_mem_buf_limit = 200\n\n# The following settings are \"undocumented\", we don't recommend uncommenting them unless directed by Microsoft.\n# They increase the maximum stdout/stderr log collection rate but will also cause higher cpu/memory usage.\n# [agent_settings.fbit_config]\n#   log_flush_interval_secs = \"1\"                 # default value is 15\n#   tail_mem_buf_limit_megabytes = \"10\"           # default value is 10\n#   tail_buf_chunksize_megabytes = \"1\"            # default value is 32kb (comment out this line for default)\n#   tail_buf_maxsize_megabytes = \"1\"              # defautl value is 32kb (comment out this line for default)","alertable-metrics-configuration-settings":"# Alertable metrics configuration settings for container resource utilization\n[alertable_metrics_configuration_settings.container_resource_utilization_thresholds]\n    # The threshold(Type Float) will be rounded off to 2 decimal points\n    # Threshold for container cpu, metric will be sent only when cpu utilization exceeds or becomes equal to the following percentage\n    container_cpu_threshold_percentage = 95.0\n    # Threshold for container memoryRss, metric will be sent only when memory rss exceeds or becomes equal to the following percentage\n    container_memory_rss_threshold_percentage = 95.0\n    # Threshold for container memoryWorkingSet, metric will be sent only when memory working set exceeds or becomes equal to the following percentage\n    container_memory_working_set_threshold_percentage = 95.0\n\n# Alertable metrics configuration settings for persistent volume utilization\n[alertable_metrics_configuration_settings.pv_utilization_thresholds]\n    # Threshold for persistent volume usage bytes, metric will be sent only when persistent volume utilization exceeds or becomes equal to the following percentage\n    pv_usage_threshold_percentage = 60.0\n\n# Alertable metrics configuration settings for completed jobs count\n[alertable_metrics_configuration_settings.job_completion_threshold]\n    # Threshold for completed job count , metric will be sent only for those jobs which were completed earlier than the following threshold\n    job_completion_threshold_time_minutes = 360","config-version":"ver1","integrations":"[integrations.azure_network_policy_manager]\n    collect_basic_metrics = false\n    collect_advanced_metrics = false","log-data-collection-settings":"# Log data collection settings\n# Any errors related to config map settings can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.\n\n[log_collection_settings]\n   [log_collection_settings.stdout]\n      # In the absense of this configmap, default value for enabled is true\n      enabled = true\n      # exclude_namespaces setting holds good only if enabled is set to true\n      # kube-system log collection is disabled by default in the absence of 'log_collection_settings.stdout' setting. If you want to enable kube-system, remove it from the following setting.\n      # If you want to continue to disable kube-system log collection keep this namespace in the following setting and add any other namespace you want to disable log collection to the array.\n      # In the absense of this configmap, default value for exclude_namespaces = [\"kube-system\"]\n      exclude_namespaces = [\"kube-system\"]\n\n   [log_collection_settings.stderr]\n      # Default value for enabled is true\n      enabled = true\n      # exclude_namespaces setting holds good only if enabled is set to true\n      # kube-system log collection is disabled by default in the absence of 'log_collection_settings.stderr' setting. If you want to enable kube-system, remove it from the following setting.\n      # If you want to continue to disable kube-system log collection keep this namespace in the following setting and add any other namespace you want to disable log collection to the array.\n      # In the absense of this cofigmap, default value for exclude_namespaces = [\"kube-system\"]\n      exclude_namespaces = [\"kube-system\"]\n\n   [log_collection_settings.env_var]\n      # In the absense of this configmap, default value for enabled is true\n      enabled = true\n   [log_collection_settings.enrich_container_logs]\n      # In the absense of this configmap, default value for enrich_container_logs is false\n      enabled = false\n      # When this is enabled (enabled = true), every container log entry (both stdout \u0026 stderr) will be enriched with container Name \u0026 container Image\n   [log_collection_settings.collect_all_kube_events]\n      # In the absense of this configmap, default value for collect_all_kube_events is false\n      # When the setting is set to false, only the kube events with !normal event type will be collected\n      enabled = false\n      # When this is enabled (enabled = true), all kube events including normal events will be collected","metric_collection_settings":"# Metrics collection settings for metrics sent to Log Analytics and MDM\n[metric_collection_settings.collect_kube_system_pv_metrics]\n  # In the absense of this configmap, default value for collect_kube_system_pv_metrics is false\n  # When the setting is set to false, only the persistent volume metrics outside the kube-system namespace will be collected\n  enabled = false\n  # When this is enabled (enabled = true), persistent volume metrics including those in the kube-system namespace will be collected","prometheus-data-collection-settings":"# Custom Prometheus metrics data collection settings\n[prometheus_data_collection_settings.cluster]\n    # Cluster level scrape endpoint(s). These metrics will be scraped from agent's Replicaset (singleton)\n    # Any errors related to prometheus scraping can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.\n\n    #Interval specifying how often to scrape for metrics. This is duration of time and can be specified for supporting settings by combining an integer value and time unit as a string value. Valid time units are ns, us (or µs), ms, s, m, h.\n    interval = \"5m\"\n\n    ## Uncomment the following settings with valid string arrays for prometheus scraping\n    #fieldpass = [\"metric_to_pass1\", \"metric_to_pass12\"]\n\n    #fielddrop = [\"metric_to_drop\"]\n\n    # An array of urls to scrape metrics from.\n    # urls = [\"http://localhost:8093/metrics\"]\n\n    # An array of Kubernetes services to scrape metrics from.\n    kubernetes_services = [\"http://jms-adapter-service.connectors:8093/metrics\"]\n\n    # When monitor_kubernetes_pods = true, replicaset will scrape Kubernetes pods for the following prometheus annotations:\n    # - prometheus.io/scrape: Enable scraping for this pod\n    # - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n    #     set this to `https` \u0026 most likely set the tls config.\n    # - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n    # - prometheus.io/port: If port is not 9102 use this annotation\n    monitor_kubernetes_pods = false\n\n    ## Restricts Kubernetes monitoring to namespaces for pods that have annotations set and are scraped using the monitor_kubernetes_pods setting.\n    ## This will take effect when monitor_kubernetes_pods is set to true\n    ##   ex: monitor_kubernetes_pods_namespaces = [\"default1\", \"default2\", \"default3\"]\n    # monitor_kubernetes_pods_namespaces = [\"default1\"]\n\n    ## Label selector to target pods which have the specified label\n    ## This will take effect when monitor_kubernetes_pods is set to true\n    ## Reference the docs at https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\n    # kubernetes_label_selector = \"env=dev,app=nginx\"\n\n    ## Field selector to target pods which have the specified field\n    ## This will take effect when monitor_kubernetes_pods is set to true\n    ## Reference the docs at https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/\n    ## eg. To scrape pods on a specific node\n    # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n\n[prometheus_data_collection_settings.node]\n    # Node level scrape endpoint(s). These metrics will be scraped from agent's DaemonSet running in every node in the cluster\n    # Any errors related to prometheus scraping can be found in the KubeMonAgentEvents table in the Log Analytics workspace that the cluster is sending data to.\n\n    #Interval specifying how often to scrape for metrics. This is duration of time and can be specified for supporting settings by combining an integer value and time unit as a string value. Valid time units are ns, us (or µs), ms, s, m, h.\n    interval = \"1m\"\n\n    ## Uncomment the following settings with valid string arrays for prometheus scraping\n\n    # An array of urls to scrape metrics from. $NODE_IP (all upper case) will substitute of running Node's IP address\n    # urls = [\"http://$NODE_IP:9103/metrics\"]\n\n    #fieldpass = [\"metric_to_pass1\", \"metric_to_pass12\"]\n\n    #fielddrop = [\"metric_to_drop\"]","schema-version":"v1"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"container-azm-ms-agentconfig","namespace":"kube-system"}}
  creationTimestamp: "2021-11-04T18:23:39Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:agent-settings: {}
        f:alertable-metrics-configuration-settings: {}
        f:config-version: {}
        f:integrations: {}
        f:log-data-collection-settings: {}
        f:metric_collection_settings: {}
        f:prometheus-data-collection-settings: {}
        f:schema-version: {}
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2021-11-04T18:23:39Z"
  name: container-azm-ms-agentconfig
  namespace: kube-system
  resourceVersion: "2329407"
  uid: cc2009c2-525d-431c-a654-2cc75ddf0a78
